<!DOCTYPE html >
<html>
        <head>
          <title>smile - SMILE - Statistical Machine Intelligence and Learning Engine - smile</title>
          <meta name="description" content="smile - SMILE - Statistical Machine Intelligence and Learning Engine - smile" />
          <meta name="keywords" content="smile SMILE Statistical Machine Intelligence and Learning Engine smile" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../lib/template.js"></script>
      <script type="text/javascript" src="../lib/tools.tooltip.js"></script>
      <script type="text/javascript" src="../lib/modernizr.custom.js"></script><script type="text/javascript" src="../lib/diagrams.js" id="diagrams-js"></script>
      <script type="text/javascript">
         if(top === self) {
            var url = '../index.html';
            var hash = 'smile.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../lib/package_big.png" />
        
        <h1>smile</h1><span class="permalink">
      <a href="../index.html#smile.package" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">smile</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        
        
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="smile.classification" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="classification"></a>
      <a id="classification:classification"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="classification/package.html"><span class="name">classification</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../index.html#smile.package@classification" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Classification algorithms.</p><div class="fullcomment"><div class="comment cmt"><p>Classification algorithms. In machine learning and pattern recognition,
classification refers to an algorithmic procedure for assigning a given
input object into one of a given number of categories. The input
object is formally termed an instance, and the categories are termed classes.</p><p>The instance is usually described by a vector of features, which together
constitute a description of all known characteristics of the instance.
Typically, features are either categorical (also known as nominal, i.e.
consisting of one of a set of unordered items, such as a gender of &quot;male&quot;
or &quot;female&quot;, or a blood type of &quot;A&quot;, &quot;B&quot;, &quot;AB&quot; or &quot;O&quot;), ordinal (consisting
of one of a set of ordered items, e.g. &quot;large&quot;, &quot;medium&quot; or &quot;small&quot;),
integer-valued (e.g. a count of the number of occurrences of a particular
word in an email) or real-valued (e.g. a measurement of blood pressure).</p><p>Classification normally refers to a supervised procedure, i.e. a procedure
that produces an inferred function to predict the output value of new
instances based on a training set of pairs consisting of an input object
and a desired output value. The inferred function is called a classifier
if the output is discrete or a regression function if the output is
continuous.</p><p>The inferred function should predict the correct output value for any valid
input object. This requires the learning algorithm to generalize from the
training data to unseen situations in a &quot;reasonable&quot; way.</p><p>A wide range of supervised learning algorithms is available, each with
its strengths and weaknesses. There is no single learning algorithm that
works best on all supervised learning problems. The most widely used
learning algorithms are AdaBoost and gradient boosting, support vector
machines, linear regression, linear discriminant analysis, logistic
regression, naive Bayes, decision trees, k-nearest neighbor algorithm,
and neural networks (multilayer perceptron).</p><p>If the feature vectors include features of many different kinds (discrete,
discrete ordered, counts, continuous values), some algorithms cannot be
easily applied. Many algorithms, including linear regression, logistic
regression, neural networks, and nearest neighbor methods, require that
the input features be numerical and scaled to similar ranges (e.g., to
the [-1,1] interval). Methods that employ a distance function, such as
nearest neighbor methods and support vector machines with Gaussian kernels,
are particularly sensitive to this. An advantage of decision trees (and
boosting algorithms based on decision trees) is that they easily handle
heterogeneous data.</p><p>If the input features contain redundant information (e.g., highly correlated
features), some learning algorithms (e.g., linear regression, logistic
regression, and distance based methods) will perform poorly because of
numerical instabilities. These problems can often be solved by imposing
some form of regularization.</p><p>If each of the features makes an independent contribution to the output,
then algorithms based on linear functions (e.g., linear regression,
logistic regression, linear support vector machines, naive Bayes) generally
perform well. However, if there are complex interactions among features,
then algorithms such as nonlinear support vector machines, decision trees
and neural networks work better. Linear methods can also be applied, but
the engineer must manually specify the interactions when using them.</p><p>There are several major issues to consider in supervised learning:</p><ul><li><b>Features:</b>
The accuracy of the inferred function depends strongly on how the input
object is represented. Typically, the input object is transformed into
a feature vector, which contains a number of features that are descriptive
of the object. The number of features should not be too large, because of
the curse of dimensionality; but should contain enough information to
accurately predict the output.
There are many algorithms for feature selection that seek to identify
the relevant features and discard the irrelevant ones. More generally,
dimensionality reduction may seek to map the input data into a lower
dimensional space prior to running the supervised learning algorithm.</li><li><b>Overfitting:</b>
Overfitting occurs when a statistical model describes random error
or noise instead of the underlying relationship. Over-fitting generally
occurs when a model is excessively complex, such as having too many
parameters relative to the number of observations. A model which has
been over-fit will generally have poor predictive performance, as it can
exaggerate minor fluctuations in the data.
The potential for overfitting depends not only on the number of parameters
and data but also the conformability of the model structure with the data
shape, and the magnitude of model error compared to the expected level
of noise or error in the data.
In order to avoid overfitting, it is necessary to use additional techniques
(e.g. cross-validation, regularization, early stopping, pruning, Bayesian
priors on parameters or model comparison), that can indicate when further
training is not resulting in better generalization. The basis of some
techniques is either (1) to explicitly penalize overly complex models,
or (2) to test the model's ability to generalize by evaluating its
performance on a set of data not used for training, which is assumed to
approximate the typical unseen data that a model will encounter.</li><li><b>Regularization:</b>
Regularization involves introducing additional information in order
to solve an ill-posed problem or to prevent over-fitting. This information
is usually of the form of a penalty for complexity, such as restrictions
for smoothness or bounds on the vector space norm.
A theoretical justification for regularization is that it attempts to impose
Occam's razor on the solution. From a Bayesian point of view, many
regularization techniques correspond to imposing certain prior distributions
on model parameters.</li><li><b>Bias-variance tradeoff:</b>
Mean squared error (MSE) can be broken down into two components:
variance and squared bias, known as the bias-variance decomposition.
Thus in order to minimize the MSE, we need to minimize both the bias and
the variance. However, this is not trivial. Therefore, there is a tradeoff
between bias and variance.
</li></ul></div></div>
    </li><li name="smile.io" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="io"></a>
      <a id="io:io"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="io/package.html"><span class="name">io</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../index.html#smile.package@io" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">I/O functions.</p>
    </li><li name="smile.plot" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="plot"></a>
      <a id="plot:plot"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="plot/package.html"><span class="name">plot</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../index.html#smile.package@plot" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Graphics &amp; plotting functions.</p>
    </li><li name="smile.regression" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="regression"></a>
      <a id="regression:regression"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="regression/package.html"><span class="name">regression</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../index.html#smile.package@regression" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Regression analysis.</p><div class="fullcomment"><div class="comment cmt"><p>Regression analysis. Regression analysis includes any
techniques for modeling and analyzing several variables, when the focus
is on the relationship between a dependent variable and one or more
independent variables. Most commonly, regression analysis estimates the
conditional expectation of the dependent variable given the independent
variables. Therefore, the estimation target is a function of the independent
variables called the regression function. Regression analysis is widely
used for prediction and forecasting.
</p></div></div>
    </li><li name="smile.util" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="util"></a>
      <a id="util:util"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="util/package.html"><span class="name">util</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../index.html#smile.package@util" title="Permalink" target="_top">
        <img src="../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Utility functions.</p>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>
